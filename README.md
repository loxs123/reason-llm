# GRPO-VLLM: Efficient LLM Fine-Tuning with Optimized Resource Utilization ğŸš€

[![GitHub Stars](https://img.shields.io/github/stars/loxs123/grpo-vllm?style=social)](https://github.com/loxs123/grpo-vllm)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

A cutting-edge framework for efficient GRPO algorithm implementation with VLLM acceleration, enabling large language model fine-tuning(7B full finetune+GRPO) on single 80GB GPU.

## ğŸŒŸ Key Features

**âš¡ Ultra-Efficient Resource Usage**
- Lower GPU memory consumption than other methods
- Serialized sampling & training pipeline for optimal GPU utilization
- Supports training with context lengths up to 8K tokens
- Batch-agnostic answer processing
- Supports Lora fine-tuning

**ğŸš€ Accelerated Performance**
- VLLM-powered sampling acceleration

**ğŸ§© Production-Ready Design**

- Simple directory structure
- DeepSpeed Zero-3 integration
- Seamless HuggingFace ecosystem compatibility

## ğŸ¯ Why GRPO-VLLM?

| Challenge                  | Conventional Solutions | Our Approach               |
|----------------------------|------------------------|----------------------------|
| High VRAM Requirements         | Dual-model loading(vllm && train) | Single GPU support        |
| Slow Sampling Speed        | Transformers processing   | VLLM GPU acceleration      |
| High Min Batch Size Per Device  | group size   | 1   |
| Memory Inefficiency        | Dual-model loading(vllm && train) | Single-model architecture  |

## ğŸ› ï¸ Getting Started

### Prerequisites
- NVIDIA GPU
- CUDA 11.8+
- Python 3.9+

### Installation
```bash
git clone https://github.com/loxs123/grpo-vllm.git
cd grpo-vllm
pip install -e .
```

### Project Structure
```bash
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ train.csv        # Training dataset
â”‚   â”œâ”€â”€ test.csv         # Test dataset
â”‚   â””â”€â”€ buffer.json      # Auto-generated training buffer
â”œâ”€â”€ model                # Model directory
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â””â”€â”€ tokenizer...
â””â”€â”€ grpo_vllm            # Core framework
    â”œâ”€â”€ grpo_config.py   # Training configuration
    â”œâ”€â”€ grpo_dataset.py  # Data processing
    â””â”€â”€ ...              # Implementation modules
```

### Dataset Format (`train.csv`)
| question                                   | answer |
|-------------------------------------------|--------|
| Mathematical problem in LaTeX...          | 60     |
| Calculus optimization problem...          | 15     |
| Algebraic equation challenge...           | 20     |

### Launch Training
```bash
python scripts/train.py
```

### Tips

1. The larger the Lora rank, the better(â‰¥128); a small Lora rank may lead to poor convergence.
2. The larger the batch size, the better.

## ğŸ“Š Experimental Results (Pending)



*Testing on [AIME 2024 Dataset](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024)*

## ğŸ§  Technical Foundation

### Core Components
1. **GRPO Algorithm** - Group Relative Policy Optimization
2. **VLLM Acceleration** - Paged Attention implementation
3. **Dynamic Batching** - Flexible sequence processing
4. **Memory Optimization** - Gradient checkpointing + DeepSpeed


## ğŸ“š References
1. [VLLM Official Implementation](https://github.com/vllm-project/vllm)
2. [DeepSeek-R1 Model](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)
3. [TRL Library](https://github.com/huggingface/trl)
4. [AIME Dataset](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)

## ğŸ¤ Contribution Roadmap
    - [ ] Complete initial benchmark results
    - [ ] load checkpoint? or only model?
---

*Empowering efficient LLM fine-tuning for everyone* ğŸ¤–
