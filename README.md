# ReasonLLM: Efficient LLM RL Fine-Tuning with Optimized Resource Utilization ğŸš€

[![GitHub Stars](https://img.shields.io/github/stars/loxs123/grpo-vllm?style=social)](https://github.com/loxs123/reason-llm)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

A cutting-edge framework for efficient GRPO algorithm implementation with VLLM acceleration, enabling large language model fine-tuning with lower GPU memory usage.

## ğŸŒŸ Key Features

**âš¡ Ultra-Efficient Resource Usage**
- Lower GPU memory consumption than other methods
- Serialized sampling & training pipeline for optimal GPU utilization
- Dynamic-Batch processing
- Supports Lora fine-tuning

**ğŸš€ Accelerated Performance**
- VLLM-powered sampling acceleration

**ğŸ§© Production-Ready Design**
- Simple directory structure
- DeepSpeed Zero-2/3 integration
- Seamless HuggingFace ecosystem compatibility


| Challenge                  | Conventional Solutions | Our Approach               |
|----------------------------|------------------------|----------------------------|
| Slow Sampling Speed        | Transformers processing   | VLLM GPU acceleration      |
| High Min Batch Size Per Device  | group size   | 1   |
| Memory Inefficiency/High VRAM Requirements | Dual-model loading(vllm/train) | Single-model loading  |

## ğŸ› ï¸ Getting Started

### Prerequisites
- NVIDIA GPU
- CUDA 12+
- Python 3.10+

### Installation
```bash
git clone https://github.com/loxs123/reason-llm.git
cd reason-llm
pip install -e . # If it fails, please install the required dependencies one by one.
# Warning: Since vLLM has strict requirements for the Torch version, installing it this way may break the original Torch environment. You may need to adjust the `requirements.txt` file as needed.
# export HF_ENDPOINT=https://hf-mirror.com # if use mirror
```

### Project Structure
```bash
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ test.csv         # Test dataset
â”‚   â””â”€â”€ buffer.json      # Auto-generated training buffer
â”œâ”€â”€ model                # Model directory
â”‚   â”œâ”€â”€ config.json      # put your model here
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â””â”€â”€ tokenizer...
â””â”€â”€ reason_llm            # Core framework
    â”œâ”€â”€ config.py         # Training configuration
    â”œâ”€â”€ reward_fn.py      # Reward Functions
    â””â”€â”€ ...              # Implementation modules
```

### Launch Training

```bash
nohup python -u scripts/train.py &
```
### Training considerations
```plain_text
config_file : `reason_llm/config.py`
config list : `configs/*.py`
In multi-GPU training, do not forget update `num_processes` in `reason_llm/deepspeed_zero3.yaml` to match the number of GPUs.
deepseek : Need to modify `tokenizer_config.json` https://zhuanlan.zhihu.com/p/21465667399
```
### Some experiences and tips.

1. The larger the Lora rank, the better(â‰¥128);

2. The larger the batch size, the better.

3. Removing samples with Advantage < 0 can lead to a better result.

4. Removing samples where reward.std() is too small (<0.1).

## ğŸ“Š Experimental Results
### Qwen2.5-7B

| Item            | detail                                         |
|---------------|--------------------------------------------|
| **Train Base Model** | [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) |
| **Train Type**  | full finetune                           |
| **Train Hardware** | 1Ã—A100(80G)                          |
| **Train Time**  | 12h                                     |
| **Train Dataset**  | [xiaodongguaAIGC/X-R1-7500](https://huggingface.co/datasets/xiaodongguaAIGC/X-R1-7500) |
| **Test Dataset**  | [AIME 2024 Dataset](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024) |
| **System Setting**  | ```A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>``` |

![å®éªŒç»“æœ](images/metrics_analysis.png)
[è®­ç»ƒæ—¥å¿—](log/log.out)
commit id:9de0d1fda962a42a9e6a6b4ed10ddf3f171dea3c

### Qwen2.5-3B

| Item            | detail                                         |
|---------------|--------------------------------------------|
| **Train Base Model** | [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) |
| **Train Type**  | full finetune                           |
| **Train Hardware** | 4Ã—3090                               |
| **Train Time**  | 5h(1 epoch)                           |
| **Train Dataset**  | [xiaodongguaAIGC/X-R1-7500](https://huggingface.co/datasets/xiaodongguaAIGC/X-R1-7500) |
| **Test Dataset**  | [AIME 2024 Dataset](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024) |
| **System Setting**  | ```A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>``` |

![å®éªŒç»“æœ](images/metrics_analysis_3b.png)
[è®­ç»ƒæ—¥å¿—](log/log2.out)

commit id:9de0d1fda962a42a9e6a6b4ed10ddf3f171dea3c

The learning performance in the first 10 steps is good, but as training progresses, the model's performance starts to fluctuate. However, its overall performance still surpasses the baseline model.


## ğŸ“š References
1. [VLLM Official Implementation](https://github.com/vllm-project/vllm)
2. [DeepSeek-R1 Model](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)
3. [TRL Library](https://github.com/huggingface/trl)
4. [AIME Dataset](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)
5. [X-R1](https://github.com/dhcode-cpp/X-R1)
---

*Empowering efficient LLM fine-tuning for everyone* ğŸ¤–
